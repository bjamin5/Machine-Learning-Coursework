{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab2 - MLP-reworked",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVL7_bgmIAPR"
      },
      "source": [
        " Backpropagation Lab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZbYjZZZ_yLV"
      },
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy.io import arff\n",
        "from numpy import argmax\n",
        "from numpy import linalg\n",
        "import math\n",
        "import copy\n",
        "import pdb\n",
        "import sklearn\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1+math.e**-x)\n",
        "\n",
        "assert sigmoid(0) == .5\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "  return x*(1-x)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCcEPx5VIORj"
      },
      "source": [
        "## 1. (40%) Correctly implement and submit your own code for the backpropagation algorithm. \n",
        "\n",
        "## Code requirements \n",
        "- Ability to create a network structure with at least one hidden layer and an arbitrary number of nodes.\n",
        "- Random weight initialization with small random weights with mean of 0 and a variance of 1.\n",
        "- Use Stochastic/On-line training updates: Iterate and update weights after each training instance (i.e. do not attempt batch updates)\n",
        "- Implement a validation set based stopping criterion.\n",
        "- Shuffle training set at each epoch.\n",
        "- Option to include a momentum term\n",
        "\n",
        "You may use your own random train/test split or use the scikit-learn version if you want.\n",
        "\n",
        "Use your Backpropagation algorithm to solve the Debug data. We provide you with several parameters, and you should be able to replicate our results every time. When you are confident it is correct, run your script on the Evaluation data with the same parameters, and print your final weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a2KSZ_7AN0G"
      },
      "source": [
        "class MLPClassifier(BaseEstimator,ClassifierMixin):\n",
        "\n",
        "    def __init__(self, hidden_layer_widths, num_input_nodes, num_outputs, lr=.1, momentum=0, shuffle=True, ):\n",
        "        \"\"\" Initialize class with chosen hyperparameters.\n",
        "        Args:\n",
        "            lr (float): A learning rate / step size.\n",
        "            shuffle(boolean): Whether to shuffle the training data each epoch. DO NOT SHUFFLE for evaluation / debug datasets.\n",
        "            momentum(float): The momentum coefficent \n",
        "        Optional Args (Args we think will make your life easier):\n",
        "            hidden_layer_widths (list(int)): A list of integers which defines the width of each hidden layer if hidden layer is none do twice as many hidden nodes as input nodes.\n",
        "        Example:\n",
        "            mlp = MLPClassifier(lr=.2,momentum=.5,shuffle=False,hidden_layer_widths = [3,3]),  <--- this will create a model with two hidden layers, both 3 nodes wide\n",
        "        \"\"\"\n",
        "        self.num_outputs = num_outputs\n",
        "        self.num_input_nodes = num_input_nodes\n",
        "        self.hidden_layer_widths = hidden_layer_widths if hidden_layer_widths else [2 * num_input_nodes] \n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.shuffle = shuffle\n",
        "        self.biases = np.ones(np.size(hidden_layer_widths) + 1)\n",
        "        self.bsf_weights = []\n",
        "        self.bsf_mse = 0\n",
        "        self.mse_list = []\n",
        "        self.num_epochs_ran = 0\n",
        "        self.avg_scores = []\n",
        "        \n",
        "    def add_validation_set(self, X_val, y_val):\n",
        "        self.X_val = X_val\n",
        "        self.y_val = y_val\n",
        "        #pdb.set_trace()\n",
        "\n",
        "    def fit(self, X, y, initial_weights_in=None, initial_weights_hi=None, random_weights = False, num_epochs = 10):\n",
        "        \"\"\" Fit the data; run the algorithm and adjust the weights to find a good solution\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
        "            y (array-like): A 2D numpy array with the training targets\n",
        "        Optional Args (Args we think will make your life easier):\n",
        "            initial_weights (array-like): allows the user to provide initial weights\n",
        "        Returns:\n",
        "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
        "        \"\"\"\n",
        "        num_inputs = self.num_input_nodes\n",
        "        num_hidden = self.hidden_layer_widths[0] + 1\n",
        "        num_outputs = self.num_outputs\n",
        "      \n",
        "        if random_weights:\n",
        "            self.weights_in, self.weights_hi = self.initialize_weights()\n",
        "        else:\n",
        "            self.weights_in = initial_weights_in\n",
        "            self.weights_hi = initial_weights_hi\n",
        "\n",
        "        prev_in_deltas = np.zeros(self.weights_in.shape)\n",
        "        prev_hi_deltas = np.zeros(self.weights_hi.shape)\n",
        "        #pdb.set_trace()\n",
        "        y_out = np.zeros((num_outputs,1))\n",
        "        z = None\n",
        "        iter = 0\n",
        "        sumerror = 0\n",
        "        avg_sum_error = 0\n",
        "        sum_mse = 0\n",
        "        delta = 0\n",
        "        num_epochs_no_improv = 0\n",
        "        sum_score = 0\n",
        "        \n",
        "        #pdb.set_trace()\n",
        "        while self.score(self.X_val, self.y_val) < .90 and iter < num_epochs:\n",
        "        #while iter < num_epochs and num_epochs_no_improv < 20 and : #Stop if no improvement in 10 epochs\n",
        "            ### ITERATE THROUGH DATA IN ORDER ###\n",
        "            \n",
        "            avg_sum_error = 0\n",
        "            sum_mse = 0\n",
        "            sum_score = 0\n",
        "            for j in range (np.shape(X)[0]):\n",
        "                x_input = X[j]\n",
        "                pdb.set_trace()\n",
        "                target = y[j]\n",
        "                sum_error = 0 \n",
        "                ### FEED FORWARD ###\n",
        "\n",
        "                ### Input Layer Activatons ###\n",
        "                input_data = X[j,:] # get just the first row of data # Already includes bias\n",
        "                weights_in_no_b = self.weights_in[:, :-1] #Splice off the last column of weights going to the bias(we don't need them in our dot product for this layer)\n",
        "                #pdb.set_trace()\n",
        "                z = np.dot(weights_in_no_b.T, x_input)\n",
        "                a_out = np.array(list(map(sigmoid, z)))\n",
        "                #pdb.set_trace()\n",
        "\n",
        "                ### Hidden Layer Activations\n",
        "                a_out = np.append(a_out, 1) # add a bias to the node\n",
        "                z = np.dot(self.weights_hi.T, a_out)\n",
        "                y_out = np.array(list(map(sigmoid, z))) # compute the final outputs\n",
        "                #pdb.set_trace()\n",
        "\n",
        "                ### Backwards Loop ###\n",
        " \n",
        "                ### Output Layer ###\n",
        "\n",
        "                d_outputs = np.zeros(num_outputs)\n",
        "                for k in range(num_outputs): # Loop through k output nodes indexing through one-hot-encoding targets\n",
        "                    # d = (t - a)* a * (1 - a)\n",
        "                    if (num_outputs == 1):\n",
        "                        d_outputs[k] = (y[j] - y_out) * y_out * (1 - y_out)\n",
        "                    else:\n",
        "                        d_outputs[k] = (y[j][k] - y_out[k]) * y_out[k] * (1 - y_out[k])\n",
        "                #pdb.set_trace()\n",
        "                ## Calculate error for Input/Hidden layer\n",
        "                sum_of_scaled_w = 0\n",
        "                hidden_layer_errors = np.zeros(num_hidden - 1) #Subtract 1 for the hidden node that doesn't need weights going into it\n",
        "                for node in range(num_hidden - 1): # loop through every hidden layer node and bias\n",
        "                    sum_of_scaled_w = 0\n",
        "                    for k in range(num_outputs): # for each node in previous layer\n",
        "                        sum_of_scaled_w += d_outputs[k] * self.weights_hi[node][k] * a_out[k] * (1 - a_out[k]) # sum up the error_k * weights_j_k * f'(z)\n",
        "                    hidden_layer_errors[node] = sum_of_scaled_w\n",
        "                \n",
        "      \n",
        "                ## Adjust output layer/hidden layer weights\n",
        "                for q in range(num_hidden):\n",
        "                    for k in range(num_outputs):\n",
        "                        #pdb.set_trace()\n",
        "                        layer_change = self.lr * a_out[q] * d_outputs[k]\n",
        "                        prev_w = prev_hi_deltas[q][k]\n",
        "                        delta = self.momentum * prev_w + layer_change\n",
        "                        self.weights_hi[q][k] += delta\n",
        "                        prev_hi_deltas[q][k] = delta\n",
        "                        \n",
        "                        #print(\"Delta {}{} : {}\".format(q, k, delta))\n",
        " \n",
        "                ## Adjust input layer weights\n",
        "                for q in range(num_inputs):\n",
        "                    for r in range(num_hidden - 1):\n",
        "                        #pdb.set_trace()\n",
        "                        layer_change = self.lr * X[j][q] * hidden_layer_errors[r]\n",
        "                        prev_w = prev_in_deltas[q][r]\n",
        "                        delta = self.momentum * prev_w + layer_change\n",
        "                        self.weights_in[q][r] += delta\n",
        "                        prev_in_deltas[q][r] = delta\n",
        "                        #pdb.set_trace()\n",
        "                        #print(\"Delta {}{} : {}\".format(q, k, delta))\n",
        "                #pdb.set_trace()\n",
        "\n",
        "                ####################################### Calculate MSE of test set\n",
        "                for index in range(len(y_out)):\n",
        "                    #pdb.set_trace()\n",
        "                    if (num_outputs == 1):\n",
        "                        sum_error += (y[j] - y_out)**2\n",
        "                    else:\n",
        "                        sum_error += (y[j][index] - y_out[index])**2\n",
        "                sum_error = sum_error / num_outputs\n",
        "                #print(sum_error)\n",
        "                sum_mse += (sum_error)\n",
        "                #######################################\n",
        "                ## Calculate the Accuracy from Test Set every iteration\n",
        "                #pdb.set_trace()\n",
        "                sum_score += self.score(self.X_val, self.y_val)\n",
        "\n",
        "                ####################################### Calculate MSE of val set\n",
        "                # outputs_from_val_set = self.predict(self.X_val)\n",
        "                # pdb.set_trace()\n",
        "                # for index in range(len(self.y_val)):\n",
        "                #     #pdb.set_trace()\n",
        "                #     sum_error += (self.y_val[index] - y_out[index])**2\n",
        "                # sum_error = sum_error / num_outputs\n",
        "                # #print(sum_error)\n",
        "                # sum_mse += (sum_error)o\n",
        "                #######################################\n",
        "\n",
        "                    \n",
        "            \n",
        "            avg_sum_error = np.mean(sum_mse)\n",
        "            self.mse_list.append(avg_sum_error)\n",
        "            self.avg_scores.append(sum_score / np.shape(X)[0])\n",
        "            #print(\"Mean Accuracy for Epoch %\", sum_score / np.shape(X)[0])\n",
        "\n",
        "            if (iter == 0):\n",
        "                self.bsf_mse = avg_sum_error\n",
        "            elif avg_sum_error < self.bsf_mse: # If the MSE is better than my previously recorded one\n",
        "                self.bsf_mse = avg_sum_error #Store bsf mse\n",
        "                self.bsf_weights = [self.weights_in, self.weights_hi] #store the bsf weights\n",
        "                num_epochs_no_improv = 0\n",
        "                #pdb.set_trace()\n",
        "            else:\n",
        "                num_epochs_no_improv += 1\n",
        "            #print('>epoch=%d, error=%.3f' % (iter + 1, avg_sum_error))\n",
        "            #print(\"# Epochs with no improvement: \", num_epochs_no_improv)\n",
        "            #pdb.set_trace()\n",
        "            if (self.shuffle == True):\n",
        "                X, y = self._shuffle_data(X,y)\n",
        "            iter += 1\n",
        "            # print(\"Layer 1 \\n\", self.weights_in)\n",
        "            # print(\"Layer 2 \\n\", self.weights_hi)\n",
        "            \n",
        "          \n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\" Predict all classes for a dataset X\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
        "        Returns:\n",
        "            array, shape (n_samples,)\n",
        "                Predicted target values per element in X.\n",
        "        \"\"\"\n",
        "        #pdb.set_trace()\n",
        "        pred = []\n",
        "        weights_in_no_b = self.weights_in[:, :-1]\n",
        "        a1 = np.dot(weights_in_no_b.T, X)\n",
        "        #a1 = X@weights_in_no_b\n",
        "        a_out = sigmoid(a1) #Compute the activations of the first layer\n",
        "        a_out = np.append(a_out, 1) # Add on a bias\n",
        "        z = np.dot(self.weights_hi.T, a_out)\n",
        "        pred.append(sigmoid(z)) #compute the activation of the hidden layer\n",
        "        \n",
        "        return pred\n",
        "\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        \"\"\" Initialize weights for perceptron. Don't forget the bias!\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        \n",
        "        ### List of matrices(numpy arrays)\n",
        "        self.weights = list()\n",
        "\n",
        "        # Input Layer to Hidden Layer Weights  \n",
        "        first_matrix = np.random.random((self.num_input_nodes, self.hidden_layer_widths[0] + 1)) \n",
        "        num_hidden_layers = self.hidden_layer_widths[0] + 1 # Last is the bias \n",
        "\n",
        "        # Hidden Layer to Output Layer\n",
        "        hidden_matrix = np.random.random((num_hidden_layers, self.num_outputs)) \n",
        "\n",
        "        return first_matrix, hidden_matrix\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\" Return accuracy of model on a given dataset. Must implement own score function.\n",
        "        Args:\n",
        "            X (array-like): A 2D numpy array with data, excluding targets\n",
        "            y (array-like): A 2D numpy array with targets\n",
        "        Returns:\n",
        "            score : float\n",
        "                Mean accuracy of self.predict(X) wrt. y.\n",
        "                In the MLP you usually have an output node for each class, you would classify by taking the \n",
        "                argmax of the activations of your output layer. Then, you can use the standard notion of \n",
        "                accuracy (# correct/total) to score your model. (MSE is also a measure of how well your model\n",
        "                 is doing, but separate from accuracy) (edited) \n",
        "        \"\"\"\n",
        "        \n",
        "        correct = 0\n",
        "        #pdb.set_trace()\n",
        "        total = np.shape(y)[0] # Total is equal to the number of cols in each row\n",
        "        for row in range (np.shape(X)[0]): # loop through each row of data X\n",
        "            #pdb.set_trace()\n",
        "            y_row = y[row]\n",
        "            pred_arr = self.predict(X[row])\n",
        "            if argmax(y_row) == argmax(pred_arr):\n",
        "                    correct += 1           \n",
        "\n",
        "        percentage = correct/total\n",
        "        #print(\"Accuracy %\", percentage)\n",
        "        return percentage\n",
        "\n",
        "    def _shuffle_data(self, X, y):\n",
        "        \"\"\" Shuffle the data! This _ prefix suggests that this method should only be called internally.\n",
        "            It might be easier to concatenate X & y and shuffle a single 2D array, rather than\n",
        "             shuffling X and y exactly the same way, independently.\n",
        "        \"\"\"\n",
        "        arrX = X\n",
        "        arry = np.array(y) \n",
        "        \n",
        "        pdb.set_trace()\n",
        "        y_len = self.num_outputs\n",
        "        joined = np.concatenate((arrX, arry),axis=1)\n",
        "        np.random.shuffle(joined)\n",
        "        n = len(joined[0])\n",
        "        X = joined[:,:n - y_len]\n",
        "        y = joined[:, n - y_len :]\n",
        "        \n",
        "        return X, y\n",
        "\n",
        "    ### Not required by sk-learn but required by us for grading. Returns the weights.\n",
        "    def get_weights(self):\n",
        "        return self.weights\n"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD9hIr-yG-a-"
      },
      "source": [
        "Testing on Back Propegation Hw"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "qe-bNC2lPDiH",
        "outputId": "d716f8c5-9875-4da6-f4f4-fc6298d74608"
      },
      "source": [
        "!curl https://raw.githubusercontent.com/bjamin5/CS472/main/datasets/hw_for_mlp_lab_test.arff?token=ANEQX3IIZPAJBII4ZKZJCJ3AHCE7O --output hw-data.arff\r\n",
        "testdata = arff.loadarff('hw-data.arff')\r\n",
        "df = pd.DataFrame(testdata[0])\r\n",
        "data = np.array(df)\r\n",
        "X = data[:,0:-1]\r\n",
        "numCols = np.shape(X)[0]\r\n",
        "X_bias = np.concatenate((X,np.ones((numCols,1))),axis=1)\r\n",
        "y = np.array([float(x) for x in data[:, 2]])\r\n",
        "mod_y = []\r\n",
        "for i in range(len(y)):\r\n",
        "  if (y[i] == 1):\r\n",
        "    mod_y.append([1,0])\r\n",
        "  else:\r\n",
        "    mod_y.append([0,1])\r\n",
        "print(X_bias)\r\n",
        "print(y)\r\n",
        "print(mod_y)\r\n",
        "w1 = np.ones((3,3))\r\n",
        "w2 = np.ones((3,1))\r\n",
        "mlp = MLPClassifier([3], 3, 1, 1, 0, False)\r\n",
        "mlp.fit(X_bias, y, w1, w2, False, 10)\r\n",
        "print(\"Final Layer: \\n\")\r\n",
        "print(\"Layer 1 \\n\", mlp.weights_in[:, :-1])\r\n",
        "print(\"Layer 2 \\n\", mlp.weights_hi)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   110  100   110    0     0    833      0 --:--:-- --:--:-- --:--:--   833\n",
            "[[0.0 0.0 1.0]\n",
            " [0.0 1.0 1.0]]\n",
            "[1. 0.]\n",
            "[[1, 0], [0, 1]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-6fdd11d455e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final Layer: \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Layer 1 \\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-90-f28ba73dfffb>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, initial_weights_in, initial_weights_hi, random_weights, num_epochs)\u001b[0m\n\u001b[1;32m    114\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                         \u001b[0;31m#pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                         \u001b[0mlayer_change\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0md_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                         \u001b[0mprev_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev_hi_deltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                         \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprev_w\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlayer_change\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 3"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOc2K97VPDic"
      },
      "source": [
        "One Hot Encoding Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHdbAgj5AXj8"
      },
      "source": [
        "Load in Debug Data to test MLP "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keL1qIdvusrn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cff453e7-7738-4b0f-9249-efe5f50c0303"
      },
      "source": [
        "num_input_nodes = np.shape(X)[1]\r\n",
        "num_input_nodes\r\n",
        "X_bias"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.4, 0.3, 1.0],\n",
              "       [-0.3, 0.8, 1.0],\n",
              "       [-0.2, 0.3, 1.0],\n",
              "       [-0.1, 0.9, 1.0],\n",
              "       [-0.1, 0.1, 1.0],\n",
              "       [0.0, -0.2, 1.0],\n",
              "       [0.1, 0.2, 1.0],\n",
              "       [0.2, -0.2, 1.0]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_DgnfK_0sAA",
        "outputId": "7c597a3a-b537-450f-9e83-0baf946202bc"
      },
      "source": [
        "!curl https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/linsep2nonorigin.arff --output debug-data.arff\r\n",
        "\r\n",
        "data = arff.loadarff('debug-data.arff')\r\n",
        "df = pd.DataFrame(data[0])\r\n",
        "#df\r\n",
        "df.head()\r\n",
        "data = np.array(df)\r\n",
        "X = data[:,0:-1]\r\n",
        "numCols = np.shape(X)[0]\r\n",
        "X_bias = np.concatenate((X,np.ones((numCols,1))),axis=1)\r\n",
        "y = np.array([float(x) for x in data[:, 2]])\r\n",
        "\r\n",
        "mod_y = []\r\n",
        "for i in range(len(y)):\r\n",
        "  if (y[i] == 1):\r\n",
        "    mod_y.append([1,0])\r\n",
        "  else:\r\n",
        "    mod_y.append([0,1])\r\n",
        "\r\n",
        "num_input_nodes = np.shape(X_bias)[1]\r\n",
        "\r\n",
        "width_of_hidden = [4] #Not Including bias \r\n",
        "num_outputs = 2\r\n",
        "\r\n",
        "w1 = np.zeros((3,5))\r\n",
        "w2 = np.zeros((5,2))\r\n",
        "initial_weights = [w1, w2]\r\n",
        "#print(\"X_bias is: \\n\", X_bias)\r\n",
        "#print(\"Num of outputs is \", num_outputs)\r\n",
        "#print(\"X is:\", X)\r\n",
        "#print(\"y is: \\n\", y)\r\n",
        "\r\n",
        "mlp = MLPClassifier(width_of_hidden, num_input_nodes, num_outputs, .1, .5, False)\r\n",
        "mlp.fit(X_bias, mod_y, w1, w2, False, 10)\r\n",
        "print(\"Layer 1 \\n\", mlp.weights_in[:, :-1])\r\n",
        "print(\"Layer 2 \\n\", mlp.weights_hi)\r\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   197  100   197    0     0   4104      0 --:--:-- --:--:-- --:--:--  4191\n",
            ">epoch=1, error=2.079\n",
            ">epoch=2, error=2.058\n",
            ">epoch=3, error=2.058\n",
            ">epoch=4, error=2.059\n",
            ">epoch=5, error=2.059\n",
            ">epoch=6, error=2.060\n",
            ">epoch=7, error=2.060\n",
            ">epoch=8, error=2.061\n",
            ">epoch=9, error=2.061\n",
            ">epoch=10, error=2.061\n",
            "Layer 1 \n",
            " [[-0.00018149 -0.00018149 -0.00018149 -0.00018149]\n",
            " [ 0.00157468  0.00157468  0.00157468  0.00157468]\n",
            " [-0.00788218 -0.00788218 -0.00788218 -0.00788218]]\n",
            "Layer 2 \n",
            " [[-0.01050642  0.01050642]\n",
            " [-0.01050642  0.01050642]\n",
            " [-0.01050642  0.01050642]\n",
            " [-0.01050642  0.01050642]\n",
            " [-0.02148778  0.02148778]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KibCIXIThpbE"
      },
      "source": [
        "## 1.1 \n",
        "\n",
        "Debug and Evaluate your model using the following parameters:\n",
        "\n",
        "Learning Rate = 0.1\\\n",
        "Momentum = 0.5\\\n",
        "Deterministic = 10 [This means run it 10 epochs and should be the same everytime you run it]\\\n",
        "Shuffle = False\\\n",
        "Validation size = 0\\\n",
        "Initial Weights = All zeros\\\n",
        "Hidden Layer Widths = [4]\n",
        "\n",
        "---\n",
        "\n",
        "### 1.1.1 Debug \n",
        "\n",
        "Debug your model by running it on the [Debug Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/linsep2nonorigin.arff)\n",
        "\n",
        "\n",
        "Expected Results for Binary Classification (i.e. 1 output node): [debug_bp_0.csv](https://github.com/cs472ta/CS472/blob/master/debug_solutions/debug_bp_0.csv) \n",
        "\n",
        "$$ \\text{Layer 1} = \\begin{bmatrix} -8.81779797\\text{e}-05 & -8.81779797\\text{e}-05 & -8.81779797\\text{e}-05 & -8.81779797\\text{e}-05 \\\\ 7.82757731\\text{e}-04 & 7.82757731\\text{e}-04 & 7.82757731\\text{e}-04 & 7.82757731\\text{e}-04 \\\\ -3.94353645\\text{e}-03 & -3.94353645\\text{e}-03 & -3.94353645\\text{e}-03 & -3.94353645\\text{e}-03 \\end{bmatrix}$$\n",
        "                                             \n",
        "$$ \\text{Layer 2} = \\begin{bmatrix} -0.01060888 \\\\ -0.01060888 \\\\ -0.01060888 \\\\ -0.01060888 \\\\ -0.02145495 \\end{bmatrix}$$\n",
        "\n",
        "(The weights do not need to be in this order or shape.)\n",
        "\n",
        "Expected Results for One Hot Vector Classification (i.e. 2 output nodes): [debug_bp_2outs.csv](https://github.com/cs472ta/CS472/blob/master/debug_solutions/debug_bp_2outs.csv) \n",
        "\n",
        "$$ \\text{Layer 1} = \\begin{bmatrix} -0.00018149 & -0.00018149 & -0.00018149 & -0.00018149 \\\\ 0.00157468 & 0.00157468 & 0.00157468 & 0.00157468 \\\\ -0.00788218 & -0.00788218 & -0.00788218 & -0.00788218 \\end{bmatrix}$$\n",
        "                          \n",
        "$$ \\text{Layer 2} = \\begin{bmatrix} 0.01050642 & -0.01050642 \\\\ 0.01050642 & -0.01050642 \\\\ 0.01050642 & -0.01050642 \\\\ 0.01050642 & -0.01050642 \\\\ 0.02148778 & -0.02148778 \\end{bmatrix}$$\n",
        "\n",
        "(The weights do not need to be in this order or shape.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY3VNB1ui03N"
      },
      "source": [
        "### 1.1.2 Evaluation\n",
        "\n",
        "We will evaluate your model based on it's performance on the [Evaluation Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/data_banknote_authentication.arff)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yAxA78QjDh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77c4bd48-374b-4bce-af3e-d17ecee10e81"
      },
      "source": [
        "# Load evaluation data\n",
        "!curl https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/data_banknote_authentication.arff --output eval-data.arff\n",
        "data = arff.loadarff('debug-data.arff')\n",
        "df = pd.DataFrame(data[0])\n",
        "data = np.array(df)\n",
        "X = data[:,0:-1]\n",
        "numCols = np.shape(X)[0]\n",
        "X_bias = np.concatenate((X,np.ones((numCols,1))),axis=1)\n",
        "y = np.array([float(x) for x in data[:, 2]])\n",
        "\n",
        "mod_y = []\n",
        "for i in range(len(y)):\n",
        "  if (y[i] == 1):\n",
        "    mod_y.append([1,0])\n",
        "  else:\n",
        "    mod_y.append([0,1])\n",
        "\n",
        "num_input_nodes = np.shape(X_bias)[1]\n",
        "width_of_hidden = [4] #Not Including bias \n",
        "num_outputs = 2\n",
        "\n",
        "w1 = np.zeros((3,5))\n",
        "w2 = np.zeros((5,2))\n",
        "initial_weights = [w1, w2]\n",
        "\n",
        "\n",
        "mlp = MLPClassifier(width_of_hidden, num_input_nodes, num_outputs, .1, .5, False)\n",
        "mlp.fit(X_bias, mod_y, None, None, False, 100)\n",
        "print(\"Layer 1 \\n\", mlp.weights_in[:, :-1])\n",
        "print(\"Layer 2 \\n\", mlp.weights_hi)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 46685  100 46685    0     0   277k      0 --:--:-- --:--:-- --:--:--  277k\n",
            ">epoch=1, error=0.269\n",
            ">epoch=2, error=0.265\n",
            ">epoch=3, error=0.262\n",
            ">epoch=4, error=0.259\n",
            ">epoch=5, error=0.257\n",
            ">epoch=6, error=0.255\n",
            ">epoch=7, error=0.254\n",
            ">epoch=8, error=0.252\n",
            ">epoch=9, error=0.251\n",
            ">epoch=10, error=0.251\n",
            "Layer 1 \n",
            " [[-0.00018149 -0.00018149 -0.00018149 -0.00018149]\n",
            " [ 0.00157468  0.00157468  0.00157468  0.00157468]\n",
            " [-0.00788218 -0.00788218 -0.00788218 -0.00788218]]\n",
            "Layer 2 \n",
            " [[-0.01050642  0.01050642]\n",
            " [-0.01050642  0.01050642]\n",
            " [-0.01050642  0.01050642]\n",
            " [-0.01050642  0.01050642]\n",
            " [-0.02148778  0.02148778]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vWiTdlbR2Xh"
      },
      "source": [
        "## 2. (10%) Backpropagation on the Iris Classification problem.\n",
        "\n",
        "Load the Iris Dataset [Iris Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/iris.arff)\n",
        "\n",
        "Parameters:\n",
        "- One layer of hidden nodes with the number of hidden nodes being twice the number of inputs.\n",
        "- Use a 80/20 split of the data for the training/test set.\n",
        "- Use a learning rate of 0.1\n",
        "- Use a validation set (15% of the training set) taken from the training set for your stopping criteria\n",
        "- Create one graph with MSE (mean squared error) over epochs from the training set and validation set\n",
        "- Create one graph with classification accuracy (% classified correctly) over epochs from the training set and validation set\n",
        "- Print out your test set accuracy\n",
        "\n",
        "The results for the different measurables should be shown with a different color, line type, etc. Typical backpropagation accuracies for the Iris data set are 85-95%.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SSoasDQSKXb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f8bc6305-8547-4182-c5c3-6d07da9dbb4e"
      },
      "source": [
        "# Iris Classification\r\n",
        "!curl https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/iris.arff --output iris-data.arff\r\n",
        "data = arff.loadarff('iris-data.arff')\r\n",
        "\r\n",
        "df = pd.DataFrame(data[0])\r\n",
        "flower_data = np.array(df)\r\n",
        "\r\n",
        "X = flower_data[:,0:-1]\r\n",
        "numCols = np.shape(X)[0]\r\n",
        "X_bias = np.concatenate((X,np.ones((numCols,1))),axis=1)\r\n",
        "y = np.array(flower_data[:, -1])\r\n",
        "\r\n",
        "for j, flower in enumerate(y):\r\n",
        "      if flower.decode('utf-8') == \"Iris-setosa\":\r\n",
        "          y[j] = 1\r\n",
        "      elif flower.decode('utf-8') == \"Iris-versicolor\":\r\n",
        "          y[j] = 2\r\n",
        "      elif flower.decode('utf-8') == \"Iris-virginica\":\r\n",
        "          y[j] = 3\r\n",
        "\r\n",
        "mod_y = []\r\n",
        "for i in range(len(y)):\r\n",
        "  if (y[i] == 1):\r\n",
        "    mod_y.append([1,0,0]) # Iris-setosa\r\n",
        "  elif (y[i] == 2):\r\n",
        "    mod_y.append([0,1,0]) # Iris-versicolor\r\n",
        "  else:\r\n",
        "    mod_y.append([0,0,1]) # Iris-virginica\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_bias, mod_y, test_size=0.20, random_state=1)\r\n",
        "X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X_train, y_train, test_size=0.15, random_state=1) # 0.25 x 0.8 = 0.2\r\n",
        "\r\n",
        "#w1 = np.zeros((3,5))\r\n",
        "#w2 = np.zeros((5,2))\r\n",
        "#initial_weights = [w1, w2]\r\n",
        "\r\n",
        "# TEST trin\r\n",
        "width_of_hidden = None\r\n",
        "num_input_nodes = X_train.shape[1] #4 Not Including bias \r\n",
        "num_outputs = 3\r\n",
        "train_mlp = MLPClassifier(width_of_hidden, num_input_nodes, num_outputs, .1, .5, False)\r\n",
        "train_mlp.add_validation_set(X_val, y_val)\r\n",
        "train_mlp.fit(X_bias, mod_y, None, None, True, 80)\r\n",
        "\r\n",
        "y_accuracy = np.array(train_mlp.avg_scores)\r\n",
        "x_epochs = np.array([i for i in range(len(train_mlp.mse_list))])\r\n",
        "train_y = np.array(train_mlp.mse_list)\r\n",
        "plt.plot(x_epochs,train_y)\r\n",
        "plt.xlabel(\"Epochs Trained\")\r\n",
        "plt.ylabel(\"MSE\")\r\n",
        "plt.show()\r\n",
        "\r\n",
        "plt.plot(x_epochs, y_accuracy)\r\n",
        "plt.xlabel(\"Epochs Trained\")\r\n",
        "plt.ylabel(\"Accuracy\")\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# Print out Test Accuracy\r\n",
        "print(\"Accuracy of Test Set:\", train_mlp.score(X_test, y_test))\r\n",
        "\r\n"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  7485  100  7485    0     0   121k      0 --:--:-- --:--:-- --:--:--  121k\n",
            "Mean Accuracy for Epoch % 0.3888888888888876\n",
            "Mean Accuracy for Epoch % 0.37814814814814796\n",
            "Mean Accuracy for Epoch % 0.33444444444444543\n",
            "Mean Accuracy for Epoch % 0.33333333333333426\n",
            "Mean Accuracy for Epoch % 0.33333333333333426\n",
            "Mean Accuracy for Epoch % 0.33333333333333426\n",
            "Mean Accuracy for Epoch % 0.33333333333333426\n",
            "Mean Accuracy for Epoch % 0.33333333333333426\n",
            "Mean Accuracy for Epoch % 0.33333333333333426\n",
            "Mean Accuracy for Epoch % 0.33333333333333426\n",
            "Mean Accuracy for Epoch % 0.33333333333333426\n",
            "Mean Accuracy for Epoch % 0.33333333333333426\n",
            "Mean Accuracy for Epoch % 0.33333333333333426\n",
            "Mean Accuracy for Epoch % 0.334074074074075\n",
            "Mean Accuracy for Epoch % 0.33518518518518614\n",
            "Mean Accuracy for Epoch % 0.3359259259259269\n",
            "Mean Accuracy for Epoch % 0.3359259259259269\n",
            "Mean Accuracy for Epoch % 0.33444444444444543\n",
            "Mean Accuracy for Epoch % 0.33370370370370467\n",
            "Mean Accuracy for Epoch % 0.33370370370370467\n",
            "Mean Accuracy for Epoch % 0.33370370370370467\n",
            "Mean Accuracy for Epoch % 0.33370370370370467\n",
            "Mean Accuracy for Epoch % 0.33370370370370467\n",
            "Mean Accuracy for Epoch % 0.33370370370370467\n",
            "Mean Accuracy for Epoch % 0.33370370370370467\n",
            "Mean Accuracy for Epoch % 0.33370370370370467\n",
            "Mean Accuracy for Epoch % 0.33370370370370467\n",
            "Mean Accuracy for Epoch % 0.33629629629629726\n",
            "Mean Accuracy for Epoch % 0.33814814814814914\n",
            "Mean Accuracy for Epoch % 0.33814814814814914\n",
            "Mean Accuracy for Epoch % 0.33703703703703797\n",
            "Mean Accuracy for Epoch % 0.33629629629629726\n",
            "Mean Accuracy for Epoch % 0.3366666666666676\n",
            "Mean Accuracy for Epoch % 0.33814814814814914\n",
            "Mean Accuracy for Epoch % 0.3414814814814825\n",
            "Mean Accuracy for Epoch % 0.34851851851851945\n",
            "Mean Accuracy for Epoch % 0.3444444444444454\n",
            "Mean Accuracy for Epoch % 0.3651851851851861\n",
            "Mean Accuracy for Epoch % 0.4103703703703712\n",
            "Mean Accuracy for Epoch % 0.47\n",
            "Mean Accuracy for Epoch % 0.46703703703703703\n",
            "Mean Accuracy for Epoch % 0.4933333333333324\n",
            "Mean Accuracy for Epoch % 0.5096296296296284\n",
            "Mean Accuracy for Epoch % 0.662222222222223\n",
            "Mean Accuracy for Epoch % 0.7122222222222235\n",
            "Mean Accuracy for Epoch % 0.7177777777777791\n",
            "Mean Accuracy for Epoch % 0.7200000000000012\n",
            "Mean Accuracy for Epoch % 0.7222222222222233\n",
            "Mean Accuracy for Epoch % 0.7259259259259269\n",
            "Mean Accuracy for Epoch % 0.7296296296296303\n",
            "Mean Accuracy for Epoch % 0.7344444444444449\n",
            "Mean Accuracy for Epoch % 0.7292592592592597\n",
            "Mean Accuracy for Epoch % 0.7322222222222226\n",
            "Mean Accuracy for Epoch % 0.7318518518518532\n",
            "Mean Accuracy for Epoch % 0.7355555555555572\n",
            "Mean Accuracy for Epoch % 0.7522222222222227\n",
            "Mean Accuracy for Epoch % 0.7781481481481481\n",
            "Mean Accuracy for Epoch % 0.7881481481481475\n",
            "Mean Accuracy for Epoch % 0.7944444444444437\n",
            "Mean Accuracy for Epoch % 0.816666666666666\n",
            "Mean Accuracy for Epoch % 0.8225925925925915\n",
            "Mean Accuracy for Epoch % 0.8185185185185174\n",
            "Mean Accuracy for Epoch % 0.8303703703703694\n",
            "Mean Accuracy for Epoch % 0.822962962962962\n",
            "Mean Accuracy for Epoch % 0.8181481481481471\n",
            "Mean Accuracy for Epoch % 0.8174074074074063\n",
            "Mean Accuracy for Epoch % 0.8177777777777767\n",
            "Mean Accuracy for Epoch % 0.8162962962962952\n",
            "Mean Accuracy for Epoch % 0.8140740740740727\n",
            "Mean Accuracy for Epoch % 0.8096296296296281\n",
            "Mean Accuracy for Epoch % 0.8103703703703689\n",
            "Mean Accuracy for Epoch % 0.8125925925925912\n",
            "Mean Accuracy for Epoch % 0.8140740740740728\n",
            "Mean Accuracy for Epoch % 0.8111111111111098\n",
            "Mean Accuracy for Epoch % 0.8088888888888875\n",
            "Mean Accuracy for Epoch % 0.8074074074074061\n",
            "Mean Accuracy for Epoch % 0.8059259259259246\n",
            "Mean Accuracy for Epoch % 0.8044444444444431\n",
            "Mean Accuracy for Epoch % 0.8040740740740727\n",
            "Mean Accuracy for Epoch % 0.8040740740740727\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Test Set: 0.7666666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIRG42TgSR4x"
      },
      "source": [
        "## 3. (10%) Working with the Vowel Dataset - Learning Rate\n",
        "\n",
        "Load the Vowel Dataset [Vowel Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/vowel.arff)\n",
        "\n",
        "- Use one layer of hidden nodes with the number of hidden nodes being twice the number of inputs.\n",
        "- Use random 80/20 splits of the data for the training/test set.\n",
        "- Use a validation set (15% of the training set) taken from the training set for your stopping criteria\n",
        "- Try some different learning rates (LR). Note that each LR will probably require a different number of epochs to learn. \n",
        "\n",
        "- For each LR you test, plot their validation's set MSE over Epochs on the same graph. Graph 4-5 different LRs and make them different enough to see a difference between them.\n",
        "\n",
        "In general, whenever you are testing a parameter such as LR, # of hidden nodes, etc., test values until no more improvement is found. For example, if 20 hidden nodes did better than 10, you would not stop at 20, but would try 40, etc., until you no longer get improvement.\n",
        "\n",
        "If you would like you may average the results of multiple initial conditions (e.g. 3) per LR, and that obviously would give more accurate results.\n",
        "\n",
        "<img src=https://raw.githubusercontent.com/cs472ta/CS472/master/images/backpropagation/backprop_val_set_MSE_vs_epochs.png width=500 height=500  align=\"left\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRAzs3yXTFuI",
        "outputId": "a03a950f-4fe2-4f36-b0bc-1f13ccf75575"
      },
      "source": [
        "from sklearn.compose import make_column_transformer\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "from numpy import array\r\n",
        "from numpy import argmax\r\n",
        "!curl https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/vowel.arff --output vowel-data.arff\r\n",
        "data = arff.loadarff('vowel-data.arff')\r\n",
        "df = pd.DataFrame(data[0])\r\n",
        "del df['Train or Test']\r\n",
        "column_trans = make_column_transformer(\r\n",
        "    (OneHotEncoder(), ['Speaker Number', 'Sex']),\r\n",
        "    remainder='passthrough')\r\n",
        "vowel_data = np.array(column_trans.fit_transform(df))\r\n",
        "X = vowel_data[:,0:-1]\r\n",
        "numCols = np.shape(X)[0]\r\n",
        "X_bias = np.concatenate((X,np.ones((numCols,1))),axis=1)\r\n",
        "X_bias = X_bias[:, 1:]\r\n",
        "y = np.array(vowel_data[:, -1])\r\n",
        "dfnew = df.Class\r\n",
        "dfnew\r\n",
        "data = np.array(dfnew)\r\n",
        "mod_y = []\r\n",
        "for i, vowel in enumerate(y):\r\n",
        "      #pdb.set_trace()\r\n",
        "      if vowel.decode('utf-8') == \"hUd\":\r\n",
        "          mod_y.append([1,0,0,0,0,0,0,0,0,0,0,0])\r\n",
        "      elif vowel.decode('utf-8') == \"hid\":\r\n",
        "          mod_y.append([0,1,0,0,0,0,0,0,0,0,0,0])\r\n",
        "      elif vowel.decode('utf-8') == \"hId\":\r\n",
        "          mod_y.append([0,0,1,0,0,0,0,0,0,0,0,0])\r\n",
        "      elif vowel.decode('utf-8') == \"hEd\":\r\n",
        "          mod_y.append([0,0,0,1,0,0,0,0,0,0,0,0])\r\n",
        "      elif vowel.decode('utf-8') == \"hed\":\r\n",
        "          mod_y.append([0,0,0,0,1,0,0,0,0,0,0,0])\r\n",
        "      elif vowel.decode('utf-8') == \"hAd\":\r\n",
        "          mod_y.append([0,0,0,0,0,1,0,0,0,0,0,0])\r\n",
        "      elif vowel.decode('utf-8') == \"had\":\r\n",
        "          mod_y.append([0,0,0,0,0,0,1,0,0,0,0,0])\r\n",
        "      elif vowel.decode('utf-8') == \"hYd\":\r\n",
        "          mod_y.append([0,0,0,0,0,0,0,1,0,0,0,0])\r\n",
        "      elif vowel.decode('utf-8') == \"hyd\":\r\n",
        "          mod_y.append([0,0,0,0,0,0,0,0,1,0,0,0])\r\n",
        "      elif vowel.decode('utf-8') == \"hOd\":\r\n",
        "          mod_y.append([0,0,0,0,0,0,0,0,0,1,0,0])\r\n",
        "      elif vowel.decode('utf-8') == \"hod\":\r\n",
        "          mod_y.append([0,0,0,0,0,0,0,0,0,0,1,0])\r\n",
        "      elif vowel.decode('utf-8') == \"hud\":\r\n",
        "          mod_y.append([0,0,0,0,0,0,0,0,0,0,0,1])\r\n"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 91402  100 91402    0     0   501k      0 --:--:-- --:--:-- --:--:--  501k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBGUn43ASiXW"
      },
      "source": [
        " X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_bias, mod_y, test_size=0.20, random_state=1)\r\n",
        " #X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X_train, y_train, test_size=0.15, random_state=1) # 0.25 x 0.8 = 0.2"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYVi8fnluOeO",
        "outputId": "42ffa92a-a54c-4e09-8e8e-0118883ce277"
      },
      "source": [
        "X_bias.shape"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(990, 27)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsk38GXKj38R"
      },
      "source": [
        "def gen_train(self, X_train, y_train, num_input_nodes = X_train.shape[1], lr=.1, width_of_hidden=777, num_outputs = 12,\r\n",
        "                          num_epochs=100):\r\n",
        "    \r\n",
        "    train_mlp = MLPClassifier(width_of_hidden, num_input_nodes, num_outputs, lr, .5, False)\r\n",
        "    pdb.set_trace()\r\n",
        "    train_mlp.add_validation_set(X_val, y_val)\r\n",
        "    train_mlp.fit(X_train, y_train, None, None, True, num_epochs)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLSXuOYsGObU"
      },
      "source": [
        "\r\n",
        "num_input_nodes = X_train.shape[1]\r\n",
        "width_of_hidden = [2 * num_input_nodes]\r\n",
        "num_outputs = 12\r\n",
        "lrs = [3,1,.9,.8,.7,.5,.1]\r\n",
        "lr_models = []\r\n",
        "val_set = [X_val, y_val]\r\n",
        "for rate in lrs:\r\n",
        "    print(\"Testing Learning Rate: \", rate)\r\n",
        "    cur_model = gen_train(X_train, y_train, num_input_nodes,  lr=rate, width_of_hidden=width_of_hidden, num_outputs=num_outputs,\r\n",
        "                           num_epochs=100)\r\n",
        "plt.figure()\r\n",
        "plt.title(\"MSE for Validation Set\")\r\n",
        "for i in range(len(lrs)):\r\n",
        "  cur_model = lr_models[i]\r\n",
        "  plt.plot(range(cur_model.clf.score(X_test, y_test)), cur_model., label=lrs[i])\r\n",
        "plt.legend()\r\n",
        "plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5U1i0nq4dyd"
      },
      "source": [
        "After I couldn't get my code to run the vowel dataset due to one-hot-encoding enduced errors, I used sckit's MLP to see the effect that the learning rates had on the error and optomization of the model. I was able to use my code for the Iris Data set but ran out of time to tweak my code for the vowel data set.\r\n",
        "After plotting the error (very close to MSE) the learning rate that was the biggest improved the fastest as expected but tapered off and didn't reach as low of a minimum as the smaller learning rate. Too small of a learning rate caused the line of error to be very linear. .1 was a good balance because it was inbetween the two and had a test set accuracy of %78. The learning weights of 1, .9, .8 suprisingly didn't run for very many epochs before the model stopped training due to the error not decreasing enough. This is probably because they overshot the minimum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA-8UwcaFvX2",
        "outputId": "24c15c0f-4701-4201-e706-e63d9d5cf650"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier as MLP\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "clf1 = MLP(random_state=1, activation='logistic', max_iter=100, verbose= 50, learning_rate_init=0.3, momentum=.5,n_iter_no_change = 10, hidden_layer_sizes=55, validation_fraction=.15)\r\n",
        "clf1.fit(X_train, y_train)\r\n",
        "print(\"Training set score: %f\" % clf1.score(X_train, y_train))\r\n",
        "print(\"Test set score: %f\" % clf1.score(X_test, y_test))\r\n",
        "\r\n",
        "clf2 = MLP(random_state=1, activation='logistic', max_iter=100, verbose= 50, learning_rate_init=1, momentum=.5,n_iter_no_change = 10, hidden_layer_sizes=55, validation_fraction=.15)\r\n",
        "clf2.fit(X_train, y_train)\r\n",
        "print(\"Training set score: %f\" % clf2.score(X_train, y_train))\r\n",
        "print(\"Test set score: %f\" % clf2.score(X_test, y_test))\r\n",
        "\r\n",
        "clf3 = MLP(random_state=1, activation='logistic', max_iter=100, verbose= 50, learning_rate_init=.9, momentum=.5,n_iter_no_change = 10, hidden_layer_sizes=55, validation_fraction=.15)\r\n",
        "clf3.fit(X_train, y_train)\r\n",
        "print(\"Training set score: %f\" % clf3.score(X_train, y_train))\r\n",
        "print(\"Test set score: %f\" % clf3.score(X_test, y_test))\r\n",
        "\r\n",
        "clf4 = MLP(random_state=1, activation='logistic', max_iter=100, verbose= 50, learning_rate_init=.8, momentum=.5,n_iter_no_change = 10, hidden_layer_sizes=55, validation_fraction=.15)\r\n",
        "clf4.fit(X_train, y_train)\r\n",
        "print(\"Training set score: %f\" % clf4.score(X_train, y_train))\r\n",
        "print(\"Test set score: %f\" % clf4.score(X_test, y_test))\r\n",
        "\r\n",
        "clf5 = MLP(random_state=1, activation='logistic', max_iter=100, verbose= 20, learning_rate_init=0.1, momentum=.5,n_iter_no_change = 10, hidden_layer_sizes=55, validation_fraction=.15)\r\n",
        "clf5.fit(X_train, y_train)\r\n",
        "print(\"Training set score: %f\" % clf5.score(X_train, y_train))\r\n",
        "print(\"Test set score: %f\" % clf5.score(X_test, y_test))\r\n",
        "\r\n"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 7.65110912\n",
            "Iteration 2, loss = 4.55370006\n",
            "Iteration 3, loss = 3.79311466\n",
            "Iteration 4, loss = 3.79546334\n",
            "Iteration 5, loss = 3.51549395\n",
            "Iteration 6, loss = 3.38214137\n",
            "Iteration 7, loss = 3.28731967\n",
            "Iteration 8, loss = 3.14051939\n",
            "Iteration 9, loss = 3.04026555\n",
            "Iteration 10, loss = 2.96199239\n",
            "Iteration 11, loss = 2.88354609\n",
            "Iteration 12, loss = 2.81771278\n",
            "Iteration 13, loss = 2.78934638\n",
            "Iteration 14, loss = 2.71898585\n",
            "Iteration 15, loss = 2.67734432\n",
            "Iteration 16, loss = 2.65653899\n",
            "Iteration 17, loss = 2.62444181\n",
            "Iteration 18, loss = 2.55191430\n",
            "Iteration 19, loss = 2.51863408\n",
            "Iteration 20, loss = 2.47558230\n",
            "Iteration 21, loss = 2.44351620\n",
            "Iteration 22, loss = 2.44197987\n",
            "Iteration 23, loss = 2.39785971\n",
            "Iteration 24, loss = 2.38655255\n",
            "Iteration 25, loss = 2.34615059\n",
            "Iteration 26, loss = 2.31524341\n",
            "Iteration 27, loss = 2.31186859\n",
            "Iteration 28, loss = 2.28593317\n",
            "Iteration 29, loss = 2.27269571\n",
            "Iteration 30, loss = 2.25091112\n",
            "Iteration 31, loss = 2.23317082\n",
            "Iteration 32, loss = 2.21286482\n",
            "Iteration 33, loss = 2.19267098\n",
            "Iteration 34, loss = 2.16909102\n",
            "Iteration 35, loss = 2.15859618\n",
            "Iteration 36, loss = 2.14826821\n",
            "Iteration 37, loss = 2.13753883\n",
            "Iteration 38, loss = 2.13908671\n",
            "Iteration 39, loss = 2.08927202\n",
            "Iteration 40, loss = 2.09592078\n",
            "Iteration 41, loss = 2.05502723\n",
            "Iteration 42, loss = 2.04733962\n",
            "Iteration 43, loss = 2.03478548\n",
            "Iteration 44, loss = 2.01253134\n",
            "Iteration 45, loss = 1.97905156\n",
            "Iteration 46, loss = 1.94567714\n",
            "Iteration 47, loss = 1.96751559\n",
            "Iteration 48, loss = 1.91004940\n",
            "Iteration 49, loss = 1.90473477\n",
            "Iteration 50, loss = 1.89868445\n",
            "Iteration 51, loss = 1.86969323\n",
            "Iteration 52, loss = 1.88854433\n",
            "Iteration 53, loss = 1.84149931\n",
            "Iteration 54, loss = 1.82763311\n",
            "Iteration 55, loss = 1.80872989\n",
            "Iteration 56, loss = 1.80629834\n",
            "Iteration 57, loss = 1.79948742\n",
            "Iteration 58, loss = 1.78470057\n",
            "Iteration 59, loss = 1.75749160\n",
            "Iteration 60, loss = 1.73845059\n",
            "Iteration 61, loss = 1.74255081\n",
            "Iteration 62, loss = 1.73897929\n",
            "Iteration 63, loss = 1.72202890\n",
            "Iteration 64, loss = 1.73933499\n",
            "Iteration 65, loss = 1.72694203\n",
            "Iteration 66, loss = 1.71802209\n",
            "Iteration 67, loss = 1.69441822\n",
            "Iteration 68, loss = 1.72237270\n",
            "Iteration 69, loss = 1.73939966\n",
            "Iteration 70, loss = 1.67613799\n",
            "Iteration 71, loss = 1.64427075\n",
            "Iteration 72, loss = 1.65630092\n",
            "Iteration 73, loss = 1.64200006\n",
            "Iteration 74, loss = 1.66158157\n",
            "Iteration 75, loss = 1.63396254\n",
            "Iteration 76, loss = 1.63319618\n",
            "Iteration 77, loss = 1.59076484\n",
            "Iteration 78, loss = 1.57378150\n",
            "Iteration 79, loss = 1.55514284\n",
            "Iteration 80, loss = 1.53868729\n",
            "Iteration 81, loss = 1.54905706\n",
            "Iteration 82, loss = 1.52472749\n",
            "Iteration 83, loss = 1.50953264\n",
            "Iteration 84, loss = 1.48550502\n",
            "Iteration 85, loss = 1.49829136\n",
            "Iteration 86, loss = 1.49062097\n",
            "Iteration 87, loss = 1.46646016\n",
            "Iteration 88, loss = 1.46689725\n",
            "Iteration 89, loss = 1.47143979\n",
            "Iteration 90, loss = 1.45920175\n",
            "Iteration 91, loss = 1.49100784\n",
            "Iteration 92, loss = 1.43626924\n",
            "Iteration 93, loss = 1.47720660\n",
            "Iteration 94, loss = 1.45145469\n",
            "Iteration 95, loss = 1.44488186\n",
            "Iteration 96, loss = 1.44489387\n",
            "Iteration 97, loss = 1.45382810\n",
            "Iteration 98, loss = 1.44685939\n",
            "Iteration 99, loss = 1.43112585\n",
            "Iteration 100, loss = 1.39556821\n",
            "Training set score: 0.465909\n",
            "Test set score: 0.459596\n",
            "Iteration 1, loss = 19.54869919\n",
            "Iteration 2, loss = 15.45863250\n",
            "Iteration 3, loss = 10.40386277\n",
            "Iteration 4, loss = 13.07432305\n",
            "Iteration 5, loss = 11.71068269\n",
            "Iteration 6, loss = 6.77709922\n",
            "Iteration 7, loss = 5.98864068\n",
            "Iteration 8, loss = 6.16316883\n",
            "Iteration 9, loss = 5.21970620\n",
            "Iteration 10, loss = 4.43412745\n",
            "Iteration 11, loss = 4.25709169\n",
            "Iteration 12, loss = 3.81685698\n",
            "Iteration 13, loss = 3.76119556\n",
            "Iteration 14, loss = 3.75947477\n",
            "Iteration 15, loss = 3.63219564\n",
            "Iteration 16, loss = 3.55611602\n",
            "Iteration 17, loss = 3.59361660\n",
            "Iteration 18, loss = 3.58478908\n",
            "Iteration 19, loss = 3.53844899\n",
            "Iteration 20, loss = 3.52573953\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 21, loss = 3.54688777\n",
            "Iteration 22, loss = 3.57586006\n",
            "Iteration 23, loss = 3.61248987\n",
            "Iteration 24, loss = 3.57523744\n",
            "Iteration 25, loss = 3.60077363\n",
            "Iteration 26, loss = 3.49352085\n",
            "Iteration 27, loss = 3.56773491\n",
            "Iteration 28, loss = 3.53966740\n",
            "Iteration 29, loss = 3.59182968\n",
            "Iteration 30, loss = 3.72630754\n",
            "Iteration 31, loss = 3.66298628\n",
            "Iteration 32, loss = 3.63488740\n",
            "Iteration 33, loss = 3.85437335\n",
            "Iteration 34, loss = 3.71031636\n",
            "Iteration 35, loss = 3.64116762\n",
            "Iteration 36, loss = 3.83800135\n",
            "Iteration 37, loss = 3.68714056\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Training set score: 0.000000\n",
            "Test set score: 0.000000\n",
            "Iteration 1, loss = 17.83595739\n",
            "Iteration 2, loss = 13.87470814\n",
            "Iteration 3, loss = 9.19638321\n",
            "Iteration 4, loss = 11.54014399\n",
            "Iteration 5, loss = 9.95651780\n",
            "Iteration 6, loss = 5.64562967\n",
            "Iteration 7, loss = 5.37921354\n",
            "Iteration 8, loss = 5.39095202\n",
            "Iteration 9, loss = 4.34724856\n",
            "Iteration 10, loss = 4.19537194\n",
            "Iteration 11, loss = 3.92441618\n",
            "Iteration 12, loss = 3.62458972\n",
            "Iteration 13, loss = 3.53841636\n",
            "Iteration 14, loss = 3.53888010\n",
            "Iteration 15, loss = 3.49087343\n",
            "Iteration 16, loss = 3.48936353\n",
            "Iteration 17, loss = 3.53311626\n",
            "Iteration 18, loss = 3.52963249\n",
            "Iteration 19, loss = 3.49428459\n",
            "Iteration 20, loss = 3.46442647\n",
            "Iteration 21, loss = 3.48284310\n",
            "Iteration 22, loss = 3.52044112\n",
            "Iteration 23, loss = 3.50839496\n",
            "Iteration 24, loss = 3.54540796\n",
            "Iteration 25, loss = 3.55694999\n",
            "Iteration 26, loss = 3.45370727\n",
            "Iteration 27, loss = 3.52027440\n",
            "Iteration 28, loss = 3.64577255\n",
            "Iteration 29, loss = 3.65481746\n",
            "Iteration 30, loss = 3.79814406\n",
            "Iteration 31, loss = 3.82117063\n",
            "Iteration 32, loss = 3.73024230\n",
            "Iteration 33, loss = 3.73155104\n",
            "Iteration 34, loss = 3.67689023\n",
            "Iteration 35, loss = 3.64705421\n",
            "Iteration 36, loss = 3.70051286\n",
            "Iteration 37, loss = 3.65603961\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Training set score: 0.000000\n",
            "Test set score: 0.000000\n",
            "Iteration 1, loss = 16.12353068\n",
            "Iteration 2, loss = 12.29676851\n",
            "Iteration 3, loss = 8.07602419\n",
            "Iteration 4, loss = 10.15585978\n",
            "Iteration 5, loss = 8.36387413\n",
            "Iteration 6, loss = 5.08538038\n",
            "Iteration 7, loss = 4.97297499\n",
            "Iteration 8, loss = 4.73741903\n",
            "Iteration 9, loss = 4.05782676\n",
            "Iteration 10, loss = 4.03004897\n",
            "Iteration 11, loss = 3.71077590\n",
            "Iteration 12, loss = 3.66672844\n",
            "Iteration 13, loss = 3.49593431\n",
            "Iteration 14, loss = 3.48556167\n",
            "Iteration 15, loss = 3.50526069\n",
            "Iteration 16, loss = 3.50008105\n",
            "Iteration 17, loss = 3.52863754\n",
            "Iteration 18, loss = 3.54375413\n",
            "Iteration 19, loss = 3.47535818\n",
            "Iteration 20, loss = 3.46799523\n",
            "Iteration 21, loss = 3.52168444\n",
            "Iteration 22, loss = 3.53676099\n",
            "Iteration 23, loss = 3.52211077\n",
            "Iteration 24, loss = 3.57708112\n",
            "Iteration 25, loss = 3.58316990\n",
            "Iteration 26, loss = 3.52221719\n",
            "Iteration 27, loss = 3.55124480\n",
            "Iteration 28, loss = 3.57953902\n",
            "Iteration 29, loss = 3.61412004\n",
            "Iteration 30, loss = 3.72444572\n",
            "Iteration 31, loss = 3.76127437\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Training set score: 0.000000\n",
            "Test set score: 0.000000\n",
            "Iteration 1, loss = 5.24431853\n",
            "Iteration 2, loss = 3.55589560\n",
            "Iteration 3, loss = 3.42828181\n",
            "Iteration 4, loss = 3.32431643\n",
            "Iteration 5, loss = 3.29703337\n",
            "Iteration 6, loss = 3.18667609\n",
            "Iteration 7, loss = 3.09105335\n",
            "Iteration 8, loss = 2.97723527\n",
            "Iteration 9, loss = 2.87149145\n",
            "Iteration 10, loss = 2.76746220\n",
            "Iteration 11, loss = 2.67188173\n",
            "Iteration 12, loss = 2.57325232\n",
            "Iteration 13, loss = 2.47108516\n",
            "Iteration 14, loss = 2.37234406\n",
            "Iteration 15, loss = 2.27998241\n",
            "Iteration 16, loss = 2.19882323\n",
            "Iteration 17, loss = 2.11911169\n",
            "Iteration 18, loss = 2.05539659\n",
            "Iteration 19, loss = 1.98469005\n",
            "Iteration 20, loss = 1.92970295\n",
            "Iteration 21, loss = 1.87568351\n",
            "Iteration 22, loss = 1.82567536\n",
            "Iteration 23, loss = 1.77013723\n",
            "Iteration 24, loss = 1.72655034\n",
            "Iteration 25, loss = 1.68074642\n",
            "Iteration 26, loss = 1.64188310\n",
            "Iteration 27, loss = 1.60211561\n",
            "Iteration 28, loss = 1.55935473\n",
            "Iteration 29, loss = 1.52567192\n",
            "Iteration 30, loss = 1.49267623\n",
            "Iteration 31, loss = 1.45383980\n",
            "Iteration 32, loss = 1.41978366\n",
            "Iteration 33, loss = 1.38821569\n",
            "Iteration 34, loss = 1.36046867\n",
            "Iteration 35, loss = 1.32539875\n",
            "Iteration 36, loss = 1.30071401\n",
            "Iteration 37, loss = 1.27133617\n",
            "Iteration 38, loss = 1.23528815\n",
            "Iteration 39, loss = 1.20383272\n",
            "Iteration 40, loss = 1.17820406\n",
            "Iteration 41, loss = 1.16142989\n",
            "Iteration 42, loss = 1.13758500\n",
            "Iteration 43, loss = 1.11061681\n",
            "Iteration 44, loss = 1.09122060\n",
            "Iteration 45, loss = 1.05414321\n",
            "Iteration 46, loss = 1.03857557\n",
            "Iteration 47, loss = 1.00637630\n",
            "Iteration 48, loss = 0.98825355\n",
            "Iteration 49, loss = 0.97225399\n",
            "Iteration 50, loss = 0.96363734\n",
            "Iteration 51, loss = 0.94586103\n",
            "Iteration 52, loss = 0.92840011\n",
            "Iteration 53, loss = 0.91641558\n",
            "Iteration 54, loss = 0.88876870\n",
            "Iteration 55, loss = 0.87064244\n",
            "Iteration 56, loss = 0.85138714\n",
            "Iteration 57, loss = 0.83891029\n",
            "Iteration 58, loss = 0.82231165\n",
            "Iteration 59, loss = 0.80366669\n",
            "Iteration 60, loss = 0.79149008\n",
            "Iteration 61, loss = 0.77516760\n",
            "Iteration 62, loss = 0.75965463\n",
            "Iteration 63, loss = 0.74739720\n",
            "Iteration 64, loss = 0.73442028\n",
            "Iteration 65, loss = 0.72027451\n",
            "Iteration 66, loss = 0.70415568\n",
            "Iteration 67, loss = 0.69707674\n",
            "Iteration 68, loss = 0.68493922\n",
            "Iteration 69, loss = 0.67096216\n",
            "Iteration 70, loss = 0.65291295\n",
            "Iteration 71, loss = 0.64223544\n",
            "Iteration 72, loss = 0.63063536\n",
            "Iteration 73, loss = 0.62088020\n",
            "Iteration 74, loss = 0.60522399\n",
            "Iteration 75, loss = 0.59792809\n",
            "Iteration 76, loss = 0.59609488\n",
            "Iteration 77, loss = 0.58042753\n",
            "Iteration 78, loss = 0.57725880\n",
            "Iteration 79, loss = 0.56656317\n",
            "Iteration 80, loss = 0.54720678\n",
            "Iteration 81, loss = 0.54861206\n",
            "Iteration 82, loss = 0.52853074\n",
            "Iteration 83, loss = 0.51645474\n",
            "Iteration 84, loss = 0.51722119\n",
            "Iteration 85, loss = 0.49899604\n",
            "Iteration 86, loss = 0.49317061\n",
            "Iteration 87, loss = 0.48010751\n",
            "Iteration 88, loss = 0.47651774\n",
            "Iteration 89, loss = 0.46184293\n",
            "Iteration 90, loss = 0.45882907\n",
            "Iteration 91, loss = 0.45209108\n",
            "Iteration 92, loss = 0.44352914\n",
            "Iteration 93, loss = 0.43240733\n",
            "Iteration 94, loss = 0.42547163\n",
            "Iteration 95, loss = 0.41925046\n",
            "Iteration 96, loss = 0.40690481\n",
            "Iteration 97, loss = 0.40068642\n",
            "Iteration 98, loss = 0.39754807\n",
            "Iteration 99, loss = 0.38145350\n",
            "Iteration 100, loss = 0.37945134\n",
            "Training set score: 0.929293\n",
            "Test set score: 0.787879\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOteTlV6S0bq"
      },
      "source": [
        "## 3.1 (5%) Working with the Vowel Dataset - Intuition\n",
        "- Discuss the effect of varying learning rates. \n",
        "- Discuss why the vowel data set might be more difficult than Iris\n",
        "    - Report both datasets' baseline accuracies and best **test** set accuracies. \n",
        "- Consider which of the vowel dataset's given input features you should actually use (Train/test, speaker, gender, ect) and discuss why you chose the ones you did.\n",
        "\n",
        "Typical backpropagation accuracies for the Vowel data set are above 75%.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmq9GSbJS8k2"
      },
      "source": [
        "After plotting and printing out the errors of MLP's with different learning rates it became apparent that such a small change in the LR and it makes a big difference. This may be one of the most important parameters for our ML models. As I said above too small of learning rate and the progress is slow, and linear. Too large of a learning rate and the progress is quick but doesn't find as low of a minumum error as a medium learning rate. \n",
        "\n",
        "The vowel set is far more difficult than the iris data set for a couple of reasons. Features of a plant are measurable and they are more distinct form eachother. Features from sound are difficult to extract and train a neural network with. To that extent there were many more features in the Vowel Dataset as we can't be sure what features will have the highest correlation to the response variable, in this case what we are predicting. The Vowel dataset also mixed catagorical data with numerical data. This required one-hot-encoding and increased our number of inputs. The more inputs we have the more input nodes and weights our model has to calculate and store. It also had 12 outputs which decreased the chances of the model guessing correctly when with the iris datat there was a 30 percent chance it could guess correctly.\n",
        "\n",
        "The Vowel test set had an accuracy of 0.78. The Iris data set had an accuracy of 76 percent. With more time I could have adjusted the parameters for the Iris data set to yeild a higher accuracy. The accuracy of the vowel test set was naturally higher because of optomizing the lr.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ayxv_fE8J3pj"
      },
      "source": [
        "## 3.2 (10%) Working with the Vowel Dataset - Hidden Layer Nodes\n",
        "\n",
        "Using the best LR you discovered, experiment with different numbers of hidden nodes.\n",
        "\n",
        "- Start with 1 hidden node, then 2, and then double them for each test until you get no more improvement in accuracy. \n",
        "- For each number of hidden nodes find the best validation set solution (in terms of validation set MSE).  \n",
        "- Create one graph with MSE for the training set and validation set on the y-axis and # of hidden nodes on the x-axis.\n",
        "- Report the final test set accuracy for every # of hidden nodes you experimented on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA8KLTnsJ3pj"
      },
      "source": [
        "A learning rate of .1 scored the best\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xZEYYbFHDZr",
        "outputId": "fd937e69-f58c-41eb-baef-7ade59b731f2"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n",
        "clf11 = MLP(random_state=1, activation='logistic', max_iter=100, verbose= 50, learning_rate_init=1, momentum=.5,n_iter_no_change = 10, hidden_layer_sizes=100, validation_fraction=.15)\r\n",
        "clf11.fit(X_train, y_train)\r\n",
        "\r\n",
        "print(\"Test set score: %f\" % clf11.score(X_test, y_test))\r\n",
        "\r\n",
        "clf13 = MLP(random_state=1, activation='logistic', max_iter=100, verbose= 50, learning_rate_init=.9, momentum=.5,n_iter_no_change = 10, hidden_layer_sizes=20, validation_fraction=.15)\r\n",
        "clf13.fit(X_train, y_train)\r\n",
        "\r\n",
        "print(\"Test set score: %f\" % clf13.score(X_test, y_test))"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 2.16104405\n",
            "Iteration 2, loss = 48.45086913\n",
            "Iteration 3, loss = 19.95229431\n",
            "Iteration 4, loss = 18.04025371\n",
            "Iteration 5, loss = 12.71379035\n",
            "Iteration 6, loss = 7.23994378\n",
            "Iteration 7, loss = 11.69363407\n",
            "Iteration 8, loss = 14.63573263\n",
            "Iteration 9, loss = 14.08560735\n",
            "Iteration 10, loss = 15.76526748\n",
            "Iteration 11, loss = 13.04171806\n",
            "Iteration 12, loss = 8.70054470\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Test set score: 0.000000\n",
            "Iteration 1, loss = 1.95839609\n",
            "Iteration 2, loss = 8.48425032\n",
            "Iteration 3, loss = 2.92829120\n",
            "Iteration 4, loss = 4.40933296\n",
            "Iteration 5, loss = 4.44127264\n",
            "Iteration 6, loss = 1.88722684\n",
            "Iteration 7, loss = 1.84330715\n",
            "Iteration 8, loss = 2.85641728\n",
            "Iteration 9, loss = 2.90482185\n",
            "Iteration 10, loss = 2.12111426\n",
            "Iteration 11, loss = 1.25086142\n",
            "Iteration 12, loss = 1.23488918\n",
            "Iteration 13, loss = 1.79004399\n",
            "Iteration 14, loss = 1.92617991\n",
            "Iteration 15, loss = 1.49983948\n",
            "Iteration 16, loss = 1.05198803\n",
            "Iteration 17, loss = 1.08294219\n",
            "Iteration 18, loss = 1.32048886\n",
            "Iteration 19, loss = 1.34558398\n",
            "Iteration 20, loss = 1.15412662\n",
            "Iteration 21, loss = 1.01538567\n",
            "Iteration 22, loss = 1.10501445\n",
            "Iteration 23, loss = 1.21446342\n",
            "Iteration 24, loss = 1.14801650\n",
            "Iteration 25, loss = 0.99017833\n",
            "Iteration 26, loss = 0.96315700\n",
            "Iteration 27, loss = 1.06226891\n",
            "Iteration 28, loss = 1.08822177\n",
            "Iteration 29, loss = 1.00189402\n",
            "Iteration 30, loss = 0.93934416\n",
            "Iteration 31, loss = 0.98600474\n",
            "Iteration 32, loss = 1.03714340\n",
            "Iteration 33, loss = 1.00078055\n",
            "Iteration 34, loss = 0.94144918\n",
            "Iteration 35, loss = 0.95631076\n",
            "Iteration 36, loss = 1.00054143\n",
            "Iteration 37, loss = 0.98851548\n",
            "Iteration 38, loss = 0.94513410\n",
            "Iteration 39, loss = 0.94540331\n",
            "Iteration 40, loss = 0.97697483\n",
            "Iteration 41, loss = 0.97429681\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Test set score: 0.366667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cFbrGZ_J3pj"
      },
      "source": [
        "## 3.3 (10%) Working with the Vowel Dataset - Momentum\n",
        "\n",
        "Try some different momentum terms using the best number of hidden nodes and LR from your earlier experiments.\n",
        "\n",
        "- Create a graph similar to step 3.2, but with momentum on the x-axis and number of epochs until validation set convergence on the y-axis.\n",
        "- You are trying to see how much momentum speeds up learning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRx2o_5-J3pj"
      },
      "source": [
        "*Discuss Momentum here*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBBmeNQ7jvcQ"
      },
      "source": [
        "## 4.1 (10%) Use the scikit-learn (SK) version of the MLP classifier on the Iris and Vowel data sets.  \n",
        "\n",
        "You do not need to go through all the steps above, nor graph results. Compare results (accuracy and learning speed) between your version and theirs for some selection of hyper-parameters. Try different hyper-parameters and comment on their effect.\n",
        "\n",
        "At a minimum, try\n",
        "\n",
        "- number of hidden nodes and layers\n",
        "- different activation functions\n",
        "- learning rate\n",
        "- regularization and parameters\n",
        "- momentum (and try nesterov)\n",
        "- early stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFQv70W2VyqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dda670b1-6680-4273-a8e3-64f913902e81"
      },
      "source": [
        "# Iris Classification\n",
        "!curl https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/iris.arff --output iris-data.arff\n",
        "data = arff.loadarff('iris-data.arff')\n",
        "\n",
        "df = pd.DataFrame(data[0])\n",
        "flower_data = np.array(df)\n",
        "\n",
        "X = flower_data[:,0:-1]\n",
        "numCols = np.shape(X)[0]\n",
        "X_bias = np.concatenate((X,np.ones((numCols,1))),axis=1)\n",
        "y = np.array(flower_data[:, -1])\n",
        "\n",
        "for j, flower in enumerate(y):\n",
        "      if flower.decode('utf-8') == \"Iris-setosa\":\n",
        "          y[j] = 1\n",
        "      elif flower.decode('utf-8') == \"Iris-versicolor\":\n",
        "          y[j] = 2\n",
        "      elif flower.decode('utf-8') == \"Iris-virginica\":\n",
        "          y[j] = 3\n",
        "\n",
        "mod_y = []\n",
        "for i in range(len(y)):\n",
        "  if (y[i] == 1):\n",
        "    mod_y.append([1,0,0]) # Iris-setosa\n",
        "  elif (y[i] == 2):\n",
        "    mod_y.append([0,1,0]) # Iris-versicolor\n",
        "  else:\n",
        "    mod_y.append([0,0,1]) # Iris-virginica\n",
        "\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_bias, mod_y, test_size=0.20, random_state=1)\n",
        "\n",
        "\n",
        "clf6 = MLP(random_state=1, activation='logistic', max_iter=100, verbose= 20, learning_rate_init=0.1, momentum=.5,n_iter_no_change = 30, hidden_layer_sizes=10, validation_fraction=.15)\n",
        "clf6.fit(X_train, y_train)\n",
        "#print(\"Training set score: %f\" % clf6.score(X_train, y_train))\n",
        "print(\"Test set score: %f\" % clf6.score(X_test, y_test))\n",
        "\n",
        "!curl https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/vowel.arff --output vowel-data.arff\n",
        "data = arff.loadarff('vowel-data.arff')\n",
        "df = pd.DataFrame(data[0])\n",
        "del df['Train or Test']\n",
        "column_trans = make_column_transformer(\n",
        "    (OneHotEncoder(), ['Speaker Number', 'Sex']),\n",
        "    remainder='passthrough')\n",
        "vowel_data = np.array(column_trans.fit_transform(df))\n",
        "X = vowel_data[:,0:-1]\n",
        "numCols = np.shape(X)[0]\n",
        "X_bias = np.concatenate((X,np.ones((numCols,1))),axis=1)\n",
        "X_bias = X_bias[:, 1:]\n",
        "y = np.array(vowel_data[:, -1])\n",
        "dfnew = df.Class\n",
        "dfnew\n",
        "data = np.array(dfnew)\n",
        "mod_y = []\n",
        "for i, vowel in enumerate(y):\n",
        "      #pdb.set_trace()\n",
        "      if vowel.decode('utf-8') == \"hUd\":\n",
        "          mod_y.append([1,0,0,0,0,0,0,0,0,0,0,0])\n",
        "      elif vowel.decode('utf-8') == \"hid\":\n",
        "          mod_y.append([0,1,0,0,0,0,0,0,0,0,0,0])\n",
        "      elif vowel.decode('utf-8') == \"hId\":\n",
        "          mod_y.append([0,0,1,0,0,0,0,0,0,0,0,0])\n",
        "      elif vowel.decode('utf-8') == \"hEd\":\n",
        "          mod_y.append([0,0,0,1,0,0,0,0,0,0,0,0])\n",
        "      elif vowel.decode('utf-8') == \"hed\":\n",
        "          mod_y.append([0,0,0,0,1,0,0,0,0,0,0,0])\n",
        "      elif vowel.decode('utf-8') == \"hAd\":\n",
        "          mod_y.append([0,0,0,0,0,1,0,0,0,0,0,0])\n",
        "      elif vowel.decode('utf-8') == \"had\":\n",
        "          mod_y.append([0,0,0,0,0,0,1,0,0,0,0,0])\n",
        "      elif vowel.decode('utf-8') == \"hYd\":\n",
        "          mod_y.append([0,0,0,0,0,0,0,1,0,0,0,0])\n",
        "      elif vowel.decode('utf-8') == \"hyd\":\n",
        "          mod_y.append([0,0,0,0,0,0,0,0,1,0,0,0])\n",
        "      elif vowel.decode('utf-8') == \"hOd\":\n",
        "          mod_y.append([0,0,0,0,0,0,0,0,0,1,0,0])\n",
        "      elif vowel.decode('utf-8') == \"hod\":\n",
        "          mod_y.append([0,0,0,0,0,0,0,0,0,0,1,0])\n",
        "      elif vowel.decode('utf-8') == \"hud\":\n",
        "          mod_y.append([0,0,0,0,0,0,0,0,0,0,0,1])\n",
        "\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_bias, mod_y, test_size=0.20, random_state=1)\n",
        "\n",
        "clf7 = MLP(random_state=1, activation='logistic', max_iter=100, verbose= 20, learning_rate_init=0.1, momentum=.5,n_iter_no_change = 30, hidden_layer_sizes=11, validation_fraction=.15)\n",
        "clf7.fit(X_train, y_train)\n",
        "#print(\"Training set score: %f\" % clf7.score(X_train, y_train))\n",
        "print(\"Test set score: %f\" % clf7.score(X_test, y_test))"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  7485  100  7485    0     0   149k      0 --:--:-- --:--:-- --:--:--  149k\n",
            "Iteration 1, loss = 2.15883950\n",
            "Iteration 2, loss = 1.92626608\n",
            "Iteration 3, loss = 1.86100562\n",
            "Iteration 4, loss = 1.82573675\n",
            "Iteration 5, loss = 1.77693144\n",
            "Iteration 6, loss = 1.70844475\n",
            "Iteration 7, loss = 1.63689281\n",
            "Iteration 8, loss = 1.57365335\n",
            "Iteration 9, loss = 1.50793949\n",
            "Iteration 10, loss = 1.43806553\n",
            "Iteration 11, loss = 1.37310426\n",
            "Iteration 12, loss = 1.30566612\n",
            "Iteration 13, loss = 1.23358713\n",
            "Iteration 14, loss = 1.15971273\n",
            "Iteration 15, loss = 1.09490028\n",
            "Iteration 16, loss = 1.05933363\n",
            "Iteration 17, loss = 1.03069751\n",
            "Iteration 18, loss = 0.98520674\n",
            "Iteration 19, loss = 0.95353991\n",
            "Iteration 20, loss = 0.93762282\n",
            "Iteration 21, loss = 0.91574708\n",
            "Iteration 22, loss = 0.89081315\n",
            "Iteration 23, loss = 0.87346298\n",
            "Iteration 24, loss = 0.84405864\n",
            "Iteration 25, loss = 0.81806593\n",
            "Iteration 26, loss = 0.80077399\n",
            "Iteration 27, loss = 0.77703115\n",
            "Iteration 28, loss = 0.75889010\n",
            "Iteration 29, loss = 0.73893118\n",
            "Iteration 30, loss = 0.71311099\n",
            "Iteration 31, loss = 0.69333559\n",
            "Iteration 32, loss = 0.66618768\n",
            "Iteration 33, loss = 0.64167129\n",
            "Iteration 34, loss = 0.61775337\n",
            "Iteration 35, loss = 0.59328517\n",
            "Iteration 36, loss = 0.57140272\n",
            "Iteration 37, loss = 0.54590507\n",
            "Iteration 38, loss = 0.52573790\n",
            "Iteration 39, loss = 0.50172163\n",
            "Iteration 40, loss = 0.48068823\n",
            "Iteration 41, loss = 0.45824072\n",
            "Iteration 42, loss = 0.43974415\n",
            "Iteration 43, loss = 0.42033826\n",
            "Iteration 44, loss = 0.40217697\n",
            "Iteration 45, loss = 0.38435414\n",
            "Iteration 46, loss = 0.36710866\n",
            "Iteration 47, loss = 0.35097478\n",
            "Iteration 48, loss = 0.33519985\n",
            "Iteration 49, loss = 0.32183438\n",
            "Iteration 50, loss = 0.30853588\n",
            "Iteration 51, loss = 0.29651537\n",
            "Iteration 52, loss = 0.28418110\n",
            "Iteration 53, loss = 0.27363957\n",
            "Iteration 54, loss = 0.26328699\n",
            "Iteration 55, loss = 0.25440371\n",
            "Iteration 56, loss = 0.24542448\n",
            "Iteration 57, loss = 0.23772375\n",
            "Iteration 58, loss = 0.22991639\n",
            "Iteration 59, loss = 0.22309071\n",
            "Iteration 60, loss = 0.21634310\n",
            "Iteration 61, loss = 0.21064923\n",
            "Iteration 62, loss = 0.20510273\n",
            "Iteration 63, loss = 0.20023828\n",
            "Iteration 64, loss = 0.19534052\n",
            "Iteration 65, loss = 0.19102364\n",
            "Iteration 66, loss = 0.18688111\n",
            "Iteration 67, loss = 0.18329345\n",
            "Iteration 68, loss = 0.17979038\n",
            "Iteration 69, loss = 0.17660378\n",
            "Iteration 70, loss = 0.17351134\n",
            "Iteration 71, loss = 0.17068599\n",
            "Iteration 72, loss = 0.16805836\n",
            "Iteration 73, loss = 0.16559781\n",
            "Iteration 74, loss = 0.16331883\n",
            "Iteration 75, loss = 0.16111431\n",
            "Iteration 76, loss = 0.15911649\n",
            "Iteration 77, loss = 0.15717401\n",
            "Iteration 78, loss = 0.15542586\n",
            "Iteration 79, loss = 0.15369929\n",
            "Iteration 80, loss = 0.15212417\n",
            "Iteration 81, loss = 0.15059574\n",
            "Iteration 82, loss = 0.14918275\n",
            "Iteration 83, loss = 0.14784546\n",
            "Iteration 84, loss = 0.14656171\n",
            "Iteration 85, loss = 0.14535903\n",
            "Iteration 86, loss = 0.14417739\n",
            "Iteration 87, loss = 0.14308534\n",
            "Iteration 88, loss = 0.14202252\n",
            "Iteration 89, loss = 0.14102831\n",
            "Iteration 90, loss = 0.14007057\n",
            "Iteration 91, loss = 0.13914360\n",
            "Iteration 92, loss = 0.13826449\n",
            "Iteration 93, loss = 0.13740248\n",
            "Iteration 94, loss = 0.13659042\n",
            "Iteration 95, loss = 0.13579960\n",
            "Iteration 96, loss = 0.13503927\n",
            "Iteration 97, loss = 0.13430810\n",
            "Iteration 98, loss = 0.13359098\n",
            "Iteration 99, loss = 0.13290556\n",
            "Iteration 100, loss = 0.13223339\n",
            "Test set score: 1.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 91402  100 91402    0     0   444k      0 --:--:-- --:--:-- --:--:--  444k\n",
            "Iteration 1, loss = 5.28941087\n",
            "Iteration 2, loss = 3.91427528\n",
            "Iteration 3, loss = 3.68198304\n",
            "Iteration 4, loss = 3.42806476\n",
            "Iteration 5, loss = 3.41170129\n",
            "Iteration 6, loss = 3.39106481\n",
            "Iteration 7, loss = 3.37714395\n",
            "Iteration 8, loss = 3.35459598\n",
            "Iteration 9, loss = 3.33897726\n",
            "Iteration 10, loss = 3.31579061\n",
            "Iteration 11, loss = 3.28527155\n",
            "Iteration 12, loss = 3.28029230\n",
            "Iteration 13, loss = 3.26607502\n",
            "Iteration 14, loss = 3.25117991\n",
            "Iteration 15, loss = 3.23308906\n",
            "Iteration 16, loss = 3.21233505\n",
            "Iteration 17, loss = 3.17599616\n",
            "Iteration 18, loss = 3.12374067\n",
            "Iteration 19, loss = 3.07798053\n",
            "Iteration 20, loss = 3.04503803\n",
            "Iteration 21, loss = 3.01922603\n",
            "Iteration 22, loss = 2.99371959\n",
            "Iteration 23, loss = 2.96300299\n",
            "Iteration 24, loss = 2.94491819\n",
            "Iteration 25, loss = 2.92797471\n",
            "Iteration 26, loss = 2.91014746\n",
            "Iteration 27, loss = 2.89990553\n",
            "Iteration 28, loss = 2.89264913\n",
            "Iteration 29, loss = 2.87928966\n",
            "Iteration 30, loss = 2.87860079\n",
            "Iteration 31, loss = 2.86806602\n",
            "Iteration 32, loss = 2.86339097\n",
            "Iteration 33, loss = 2.85342613\n",
            "Iteration 34, loss = 2.84784815\n",
            "Iteration 35, loss = 2.84361626\n",
            "Iteration 36, loss = 2.84185771\n",
            "Iteration 37, loss = 2.83380228\n",
            "Iteration 38, loss = 2.83201206\n",
            "Iteration 39, loss = 2.82767621\n",
            "Iteration 40, loss = 2.81580566\n",
            "Iteration 41, loss = 2.80547590\n",
            "Iteration 42, loss = 2.79342716\n",
            "Iteration 43, loss = 2.76630297\n",
            "Iteration 44, loss = 2.73231628\n",
            "Iteration 45, loss = 2.68252821\n",
            "Iteration 46, loss = 2.63896399\n",
            "Iteration 47, loss = 2.57954666\n",
            "Iteration 48, loss = 2.53600461\n",
            "Iteration 49, loss = 2.48975982\n",
            "Iteration 50, loss = 2.44216861\n",
            "Iteration 51, loss = 2.41407765\n",
            "Iteration 52, loss = 2.38675220\n",
            "Iteration 53, loss = 2.36319579\n",
            "Iteration 54, loss = 2.33665194\n",
            "Iteration 55, loss = 2.31939322\n",
            "Iteration 56, loss = 2.29281566\n",
            "Iteration 57, loss = 2.28391764\n",
            "Iteration 58, loss = 2.27162986\n",
            "Iteration 59, loss = 2.26294420\n",
            "Iteration 60, loss = 2.25992889\n",
            "Iteration 61, loss = 2.24083183\n",
            "Iteration 62, loss = 2.22735966\n",
            "Iteration 63, loss = 2.22358847\n",
            "Iteration 64, loss = 2.20849537\n",
            "Iteration 65, loss = 2.19658493\n",
            "Iteration 66, loss = 2.18606088\n",
            "Iteration 67, loss = 2.18567010\n",
            "Iteration 68, loss = 2.17108851\n",
            "Iteration 69, loss = 2.14568371\n",
            "Iteration 70, loss = 2.12928755\n",
            "Iteration 71, loss = 2.09908667\n",
            "Iteration 72, loss = 2.07576616\n",
            "Iteration 73, loss = 2.04680619\n",
            "Iteration 74, loss = 2.02135963\n",
            "Iteration 75, loss = 1.99437922\n",
            "Iteration 76, loss = 1.96939130\n",
            "Iteration 77, loss = 1.94405101\n",
            "Iteration 78, loss = 1.91153972\n",
            "Iteration 79, loss = 1.88764097\n",
            "Iteration 80, loss = 1.84574629\n",
            "Iteration 81, loss = 1.82435876\n",
            "Iteration 82, loss = 1.79685039\n",
            "Iteration 83, loss = 1.76104139\n",
            "Iteration 84, loss = 1.73645366\n",
            "Iteration 85, loss = 1.71373696\n",
            "Iteration 86, loss = 1.70471376\n",
            "Iteration 87, loss = 1.66978147\n",
            "Iteration 88, loss = 1.64976154\n",
            "Iteration 89, loss = 1.63422177\n",
            "Iteration 90, loss = 1.61568060\n",
            "Iteration 91, loss = 1.58844288\n",
            "Iteration 92, loss = 1.57857298\n",
            "Iteration 93, loss = 1.57567873\n",
            "Iteration 94, loss = 1.56239735\n",
            "Iteration 95, loss = 1.53628615\n",
            "Iteration 96, loss = 1.52717144\n",
            "Iteration 97, loss = 1.51799657\n",
            "Iteration 98, loss = 1.49432796\n",
            "Iteration 99, loss = 1.49091527\n",
            "Iteration 100, loss = 1.48354328\n",
            "Test set score: 0.434343\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqSFAXwlk3Ms"
      },
      "source": [
        "I experimented with the number of hidden layers and got the following results.\r\n",
        " Iris Data: 11 Layers -> 1.0\r\n",
        "Vowel Data: 11 Layers -> .43\r\n",
        "I wanted to see what happend to the accuracy as I compressed the data vs expanded it. In the case of the Iris data set its accuracy increased a lot but it definetally overfit the data. \r\n",
        "Another observation I had was the when using early stopping, higher learning rates and very low ones would exit sooner because they either passed the minimum or they didn't progress fast enough. \r\n",
        "\r\n",
        "          \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsStkKcgJ3pk"
      },
      "source": [
        "## 4.2 (5%) Using the Iris Dataset automatically adjust hyper-parameters using your choice of grid/random search\n",
        "- Use a grid or random search approach across a reasonable subset of hyper-parameters from the above \n",
        "- Report your best accuracy and hyper-parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ8MhifxJ3pk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "862980f8-dd1f-4e0b-b8ae-65c57c799885"
      },
      "source": [
        "# Iris Classification\n",
        "!curl https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/iris.arff --output iris-data.arff\n",
        "data = arff.loadarff('iris-data.arff')\n",
        "\n",
        "df = pd.DataFrame(data[0])\n",
        "flower_data = np.array(df)\n",
        "\n",
        "X = flower_data[:,0:-1]\n",
        "numCols = np.shape(X)[0]\n",
        "X_bias = np.concatenate((X,np.ones((numCols,1))),axis=1)\n",
        "y = np.array(flower_data[:, -1])\n",
        "\n",
        "for j, flower in enumerate(y):\n",
        "      if flower.decode('utf-8') == \"Iris-setosa\":\n",
        "          y[j] = 1\n",
        "      elif flower.decode('utf-8') == \"Iris-versicolor\":\n",
        "          y[j] = 2\n",
        "      elif flower.decode('utf-8') == \"Iris-virginica\":\n",
        "          y[j] = 3\n",
        "\n",
        "mod_y = []\n",
        "for i in range(len(y)):\n",
        "  if (y[i] == 1):\n",
        "    mod_y.append([1,0,0]) # Iris-setosa\n",
        "  elif (y[i] == 2):\n",
        "    mod_y.append([0,1,0]) # Iris-versicolor\n",
        "  else:\n",
        "    mod_y.append([0,0,1]) # Iris-virginica\n",
        "\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_bias, mod_y, test_size=0.20, random_state=1)\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "mlp = MLPClassifier(max_iter=100)\n",
        "parameter_space = {\n",
        "    'hidden_layer_sizes': [(5,5,5), (3,3), (8,)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'alpha': [0.0001, 0.05],\n",
        "    'learning_rate': ['constant','adaptive'],\n",
        "}\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
        "clf.fit(X_train, y_train)\n",
        "# Best paramete set\n",
        "print('Best parameters found:\\n', clf.best_params_)\n",
        "\n",
        "# All results\n",
        "means = clf.cv_results_['mean_test_score']\n",
        "stds = clf.cv_results_['std_test_score']\n",
        "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  7485  100  7485    0     0  40901      0 --:--:-- --:--:-- --:--:-- 40901\n",
            "Best parameters found:\n",
            " {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (8,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.217 (+/-0.613) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (5, 5, 5), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.000 (+/-0.000) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (5, 5, 5), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.000 (+/-0.000) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (5, 5, 5), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.192 (+/-0.542) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (5, 5, 5), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.100 (+/-0.283) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (3, 3), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.258 (+/-0.392) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (3, 3), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.308 (+/-0.024) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (3, 3), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.333 (+/-0.047) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (3, 3), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.208 (+/-0.589) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (8,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.275 (+/-0.430) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (8,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.183 (+/-0.262) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (8,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.050 (+/-0.082) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (8,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.200 (+/-0.283) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (5, 5, 5), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.225 (+/-0.248) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (5, 5, 5), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.000 (+/-0.000) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (5, 5, 5), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.250 (+/-0.356) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (5, 5, 5), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.000 (+/-0.000) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (3, 3), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.125 (+/-0.319) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (3, 3), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.150 (+/-0.424) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (3, 3), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.017 (+/-0.047) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (3, 3), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.317 (+/-0.450) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (8,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.117 (+/-0.330) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (8,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.325 (+/-0.531) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (8,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.200 (+/-0.108) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (8,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.283 (+/-0.085) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (5, 5, 5), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.158 (+/-0.448) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (5, 5, 5), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.325 (+/-0.041) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (5, 5, 5), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.183 (+/-0.246) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (5, 5, 5), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.358 (+/-0.592) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (3, 3), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.283 (+/-0.379) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (3, 3), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.208 (+/-0.295) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (3, 3), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.133 (+/-0.193) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (3, 3), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.317 (+/-0.531) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (8,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.233 (+/-0.330) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (8,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.517 (+/-0.189) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (8,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.283 (+/-0.047) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (8,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.117 (+/-0.330) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (5, 5, 5), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.150 (+/-0.248) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (5, 5, 5), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.083 (+/-0.236) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (5, 5, 5), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.308 (+/-0.473) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (5, 5, 5), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.075 (+/-0.212) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (3, 3), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.300 (+/-0.141) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (3, 3), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.167 (+/-0.239) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (3, 3), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.175 (+/-0.227) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (3, 3), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
            "0.542 (+/-0.239) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (8,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
            "0.358 (+/-0.170) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (8,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
            "0.392 (+/-0.572) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (8,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
            "0.375 (+/-0.268) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (8,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTlK-kijk8Mg"
      },
      "source": [
        "## 5. (Optional 5% Extra credit) For the vowel data set, use the other hyper-parameter approach that you did not use in part 4.2 to find LR, # of hidden nodes, and momentum.  \n",
        "\n",
        "- Compare and discuss the values found with the ones you found in part 3.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UhG4HHrJ3pk"
      },
      "source": [
        "# Optional grid and random search"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHCbTIlSJ3pl"
      },
      "source": [
        "*Discuss findings here*"
      ]
    }
  ]
}